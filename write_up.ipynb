{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660515ba",
   "metadata": {},
   "source": [
    "# BBC Tech Screening - Principal Data Scientist\n",
    "\n",
    "For this Technical screening I have chosen to use the Movielens dataset to create a personalised movie recommender system. This is a multi-model recommender that combines an approximate nearest neighbors look up of semantically similiar movie titles and genres to a base query as a form of candidate item filtering. Then a colaborative filtering model was trained on all of the user interactions and the list of movie candidates is reranked and deduplicated of movies a user has seen before to create a personalised list of recommendations.\n",
    "\n",
    "I have packaged all of this up into a single recommender class which you can import and run the *print_recs* method on with any integer input for userID and string to specify the movie look ups, like what are in the below example *personalisedRecommender.print_recs(42, \"Horror films with zombies\")*.\n",
    "\n",
    "I'm going to start off with this example, then walk backwards through the components of this project and wrap up with an evaluation and recommendations for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7532463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the personalisedSearcher class\n",
    "from recommender.recommender import personalisedSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc5ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe52031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8401c0e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The scann library is not present. Please install it using `pip install scann` to use the ScaNN layer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b1f610a25b08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instantiate an instance of it, this will take a few moments as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# in the initialization it loads into memory all of the requisite data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpersonalisedRecommender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersonalisedSearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/ai/tech-screening-2021/recommender/recommender.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m         self.scann = tfrs.layers.factorized_top_k.ScaNN(num_leaves=1000, \n\u001b[1;32m     17\u001b[0m                                                         \u001b[0mnum_leaves_to_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                         k = round(np.sqrt(len(self.item_tensor))))\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers/LaBSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/personalized_search/lib/python3.6/site-packages/tensorflow_recommenders/layers/factorized_top_k.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, query_model, k, distance_measure, num_leaves, num_leaves_to_search, dimensions_per_block, num_reordering_candidates, parallelize_batch_searches, name)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAVE_SCANN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       raise ImportError(\n\u001b[0;32m--> 635\u001b[0;31m           \u001b[0;34m\"The scann library is not present. Please install it using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m           \"`pip install scann` to use the ScaNN layer.\")\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: The scann library is not present. Please install it using `pip install scann` to use the ScaNN layer."
     ]
    }
   ],
   "source": [
    "# Instantiate an instance of it, this will take a few moments as\n",
    "# in the initialization it loads into memory all of the requisite data\n",
    "personalisedRecommender = personalisedSearcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95645cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the recommendations can be generated for any userid and request string.\n",
    "# Below you can see the example for user 42's personalised recommendations\n",
    "# for the request for \"Horror films with Zombies\".\n",
    "personalisedRecommender.print_recs(42, \"Horror films with zombies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d9408",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "I'll now go through this project from the beginning, highlighting my choices and how I made them. First thing was to map out generally how I would accomplish this. The below workflow illustrates the final structure I came up with, one model to generate embeddings from the text and metadata around the movie and generate a candidate items from this look up, the other to learn the user embeddings to refine these lists and offer a personalised experience. All of the data required to replicate this should be available in the correct subdirectories, but the entire project can be run from scratch by running the setup command in the make file to create and install the requirements in a virtual environment and download the data. The embeddings and collaborative filtering model can then be generated by running python *src/cf.py* and *src/nlp.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42513a",
   "metadata": {},
   "source": [
    "![alt](diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e8947",
   "metadata": {},
   "source": [
    "## NLP embedding generation\n",
    "\n",
    "**All of the code I will be walking through for this section can be found in *src/nlp.py*.**\n",
    "\n",
    "I used the sentence-encoder Huggingface model \"LaBSE\" which stands for \"Language Agnostic BERT Sentence Encoder\" to create the embeddings for the items, this was for two reasons, I knew there were non-English titles and this would handle those cases, and it is the most used use sentence-encoder model, which provides some assurance on its robustness.\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "data = pd.read_csv(\"../ml-25m/movies.csv\")\n",
    "```\n",
    "\n",
    "The data has two useful fields, the title (with date) and the genres attached to each title. There isn't a good reason to encode the pipes in the genres or to keep the parenthesis in the title, so I will remove each of these.\n",
    "\n",
    "\n",
    "```python \n",
    ">>> data.head()\n",
    "   movieId                               title                                       genres\n",
    "0        1                    Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
    "1        2                      Jumanji (1995)                   Adventure|Children|Fantasy\n",
    "2        3             Grumpier Old Men (1995)                               Comedy|Romance\n",
    "3        4            Waiting to Exhale (1995)                         Comedy|Drama|Romance\n",
    "4        5  Father of the Bride Part II (1995)                                       Comedy\n",
    "```\n",
    "\n",
    "I removed the useless characters in both fields and replaced the pipes with spaces so I didn't end up with a long string but instead three strings, then created an input vector for the model by concatenating the remaining information for each item together. \n",
    "\n",
    "```python \n",
    "def remove_pars(x):\n",
    "    x = str(x)\n",
    "    return re.sub('[()]', \"\", x)\n",
    "\n",
    "def remove_pipes(x):\n",
    "    x = str(x)\n",
    "    return re.sub('\\|', \" \", x)\n",
    "\n",
    "def remove_nulls(a, b, i):\n",
    "    string_m = a[i] + \" \" + b[i]\n",
    "    return re.sub(\"\\(no genres listed\\)\", \"\", string_m)\n",
    "\n",
    "# process the titles and genres\n",
    "titles = [remove_pars(i) for i in data['title']]\n",
    "genres = [remove_pipes(i) for i in data['genres']]\n",
    "\n",
    "# make a list of the input strings for each item from these bits of data. \n",
    "input_string = [remove_nulls(titles, genres, i) for i in range(len(genres))]\n",
    "```\n",
    "\n",
    "Lastly, I loaded the model on to a GPU and iterated over each of the strings with the huggingface tokenizer to create the sentence embedding. These were then extracted from the output tensor and saved to be loaded in to the recommender later. \n",
    "\n",
    "```python\n",
    "# this will be using a GPU to speed things up \n",
    "# but will default to CPU if no devices are available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Create embeddings for each item \n",
    "embeddings_list = []\n",
    "for _, i in enumerate(input_string):\n",
    "    encoded_input = tokenizer(i, padding=True, truncation=True, max_length=64, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    embeddings = model_output.pooler_output\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    embeddings_list.append(embeddings)\n",
    "    if _ % 10000  == 0:\n",
    "        print(str(_))\n",
    "        \n",
    "# extract the embeddings\n",
    "embeddings_list_tensors = []\n",
    "for i in embeddings_list:\n",
    "    d = i.cpu()[0].numpy()\n",
    "    embeddings_list_tensors.append(d)\n",
    "\n",
    "# save them to local file. \n",
    "embeddings = pd.DataFrame(np.vstack(embeddings_list_tensors))\n",
    "embeddings.to_csv(\"../embeddings/data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2dc16",
   "metadata": {},
   "source": [
    "## Collaborative Filtering model training\n",
    "\n",
    "**All of the code I will be walking through for this section can be found in *src/cf.py*.**\n",
    "\n",
    "First the data needs some preprocessing. The data is read into memory and the movie and user id's were deduplicated and then a new mapping was applied to join the index ID to the encoded ID. I then shuffled the data for fairer sampling then split the data 90/10 training test.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"../ml-25m/ratings.csv\")\n",
    "\n",
    "user_ids = df[\"userId\"].unique().tolist()\n",
    "movie_ids = df[\"movieId\"].unique().tolist()\n",
    "\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n",
    "movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n",
    "df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n",
    "df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "x = df[[\"user\", \"movie\"]].values\n",
    "y = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n",
    "\n",
    "train_indices = int(0.9 * df.shape[0])\n",
    "x_train, x_val, y_train, y_val = (\n",
    "    x[:train_indices],\n",
    "    x[train_indices:],\n",
    "    y[:train_indices],\n",
    "    y[train_indices:],\n",
    ")\n",
    "```\n",
    "\n",
    "I used a battle tested colaborative filtering architecture and chose an embedding space of 128. The choice of Keras was for pragmatic reasons, for getting something robustly built quickly Keras is a simple option. \n",
    "\n",
    "```python\n",
    "# shape the neural colaborative filtering network\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "class RecommenderNet(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        # The sigmoid activation forces the rating to between 0 and 1\n",
    "        return tf.nn.sigmoid(x)\n",
    "```\n",
    "\n",
    "Lastly instantiate, compile, train and then save the model out to be loaded in later for making predictions on the candidate items per user. I played with three parameters here, the optimizer, the learning rate and the batch size. I tried SGD, Adam, NAdam and Adamax. Adam provided the smoothest loss curve so stayed with that, the learning rate I started with 0.01 and reduced it by orders of 10 until I found a LR that didn't start to vary the loss with epochs. I did try learning rate schedulers, but didn't find a stable method. Lastly, the batch size was set quite large to maintain training time under 1 hour, on a T4 GPU this took approximately 50 minutes. \n",
    "\n",
    "```python\n",
    "# instantiate the model\n",
    "model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=4096,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "\n",
    "# save the model after training. \n",
    "model.save('../CF')\n",
    "```\n",
    "\n",
    "The loss curve looks sensible and like the model is converging, I am happy with this model at these parameters. \n",
    "\n",
    "\n",
    "![Training vs Validation Loss](loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473356e",
   "metadata": {},
   "source": [
    "## Evaluate the component performance\n",
    "\n",
    "To start with I wanted to look at the performance of the embeddings to return appropriate movies to a given query. To do that I indexed the embeddings in a SCANN model, created a new embedding of the same length as each item from a given test query and looked at the resulting movies. \n",
    "\n",
    "```python\n",
    "scann = tfrs.layers.factorized_top_k.ScaNN(num_leaves=1000, \n",
    "                                           num_leaves_to_search = 100, \n",
    "                                           k = round(np.sqrt(len(item_tensor))))\n",
    "scann.index(item_tensor)\n",
    "\n",
    "test = \"Horror films with zombies\"\n",
    "encoded_input = tokenizer(test, padding=True, truncation=True, max_length=64, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "query = model_output.pooler_output\n",
    "query = torch.nn.functional.normalize(embeddings)\n",
    "\n",
    "test_case = scann(np.array(query.cpu()))\n",
    "data.iloc[test_case[1].numpy()[0]][0:9]\n",
    "```\n",
    "\n",
    "For the base query of \"Horror films with zombies\" I got 250 movies returned, the top ten of which are printed below. From an initial examination they look pretty decent! Zombie is in all of the titles, Horror is present in the genre tags, I don't know yet if this is the optimal solution, but it is atleast a sensible starting place.\n",
    "\n",
    "\n",
    "```python\n",
    " \tmovieId \ttitle \tgenres\n",
    "11068 \t47980 \tBio Zombie (Sun faa sau si) (1998) \tComedy|Horror\n",
    "13822 \t71535 \tZombieland (2009) \tAction|Comedy|Horror\n",
    "46049 \t171651 \tRedneck Zombies (1989) \tHorror\n",
    "23643 \t118810 \tZombie Women of Satan (2009) \tComedy|Horror\n",
    "45150 \t169738 \tZombie Wars (2006) \tHorror\n",
    "55180 \t191327 \tTeenage Zombies (1960) \tHorror|Sci-Fi\n",
    "41540 \t161912 \tZombie Night (2003) \tComedy|Horror|Sci-Fi\n",
    "23642 \t118808 \tZombie Reanimation (2009) \tAction|Comedy|Horror\n",
    "14427 \t75404 \tZMD: Zombies of Mass Destruction (2009) \tComedy|Horror\n",
    "```\n",
    "\n",
    "Next I performed a similar sense check on the collaborative filtering recommender. I looked at the recommendations for a random user from their user embedding, then filtered off the films they have seen before.\n",
    "\n",
    "```python\n",
    "recs = model.predict(user_movie_array).flatten()\n",
    "top_ratings_indices = recs.argsort()[-10:][::-1]\n",
    "recommended_movie_ids = [movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices]\n",
    "```\n",
    "For user 136160 their highest rated films were the five below, and from that user history the next 10 are recommended. To me, this looks like a sensible set of recommendations, similar genres are present, movies geared to an older audience, slightly more mature themes. This looks like it has created reasonable user embeddings and has correctly understood something about the user activity in its training. \n",
    "\n",
    "```latex\n",
    "Showing recommendations for user: 136160\n",
    "====================================\n",
    "Movies with high ratings from user\n",
    "--------------------------------\n",
    "GoldenEye (1995) : Action|Adventure|Thriller\n",
    "Twelve Monkeys (a.k.a. 12 Monkeys) (1995) : Mystery|Sci-Fi|Thriller\n",
    "From Dusk Till Dawn (1996) : Action|Comedy|Horror|Thriller\n",
    "Batman Forever (1995) : Action|Adventure|Comedy|Crime\n",
    "Robin Hood: Men in Tights (1993) : Comedy\n",
    "--------------------------------\n",
    "Top movie recommendations\n",
    "--------------------------------\n",
    "Usual Suspects, The (1995) : Crime|Mystery|Thriller\n",
    "Shawshank Redemption, The (1994) : Crime|Drama\n",
    "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964) : Comedy|War\n",
    "Godfather, The (1972) : Crime|Drama\n",
    "Rear Window (1954) : Mystery|Thriller\n",
    "One Flew Over the Cuckoo's Nest (1975) : Drama\n",
    "12 Angry Men (1957) : Drama\n",
    "Godfather: Part II, The (1974) : Crime|Drama\n",
    "Seven Samurai (Shichinin no samurai) (1954) : Action|Adventure|Drama\n",
    "Fight Club (1999) : Action|Crime|Drama|Thriller\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344acf6",
   "metadata": {},
   "source": [
    "## Putting it all together.\n",
    "\n",
    "**In *recommender/recommender.py* you can find the code for the next section**\n",
    "\n",
    "I combined the implementation of both of the above models into one personalisedSearcher class. The first step was to load all of the embeddings and models into the \\_\\_init__ of the class and create the SCANN index. \n",
    "\n",
    "```python\n",
    "class personalisedSearcher:\n",
    "    def __init__(self):\n",
    "        self.movies = pd.read_csv(\"ml-25m/movies.csv\")\n",
    "        self.ratings = pd.read_csv(\"ml-25m/ratings.csv\")\n",
    "        self.embeddings = pd.read_csv(\"embeddings/data.csv\", index_col=0)\n",
    "        self.item_tensor = tf.convert_to_tensor(self.embeddings, dtype=tf.float32)\n",
    "        self.scann = tfrs.layers.factorized_top_k.ScaNN(num_leaves=1000, \n",
    "                                                        num_leaves_to_search = 100, \n",
    "                                                        k = round(np.sqrt(len(self.item_tensor))))\n",
    "        self.scann.index(self.item_tensor)\n",
    "        self.model = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "        self.recommender = keras.models.load_model('CF')\n",
    "```\n",
    "\n",
    "I wrote a couple of helper functions to process the data along the way, create lists of the user history item indices, generate the candidates from the query string and then deduplicate it with their watching history and create the movie array of candidates. This culminates in the following prediction step and reranking of the candidates based on the user embedding. \n",
    "\n",
    "```python\n",
    "def personalised_search(self, user_id, query):\n",
    "        movie_array, movies_not_watched, movies_watched_by_user = self.filter_candidates(user_id, query)\n",
    "        scored_items = self.recommender.predict(movie_array).flatten()\n",
    "        top_rated = scored_items.argsort()[-10:][::-1]\n",
    "        _, movie_encoded2movie = self.get_movie_encodings()\n",
    "        recommended_movie_ids = [movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_rated]\n",
    "        \n",
    "        return recommended_movie_ids, movies_watched_by_user\n",
    "```\n",
    "\n",
    "This is subsequently wrapped up the *print_recs()* method which takes in the userID and query string, feeds them back down the stack of helper functions to generate the recommendations and rerank them then prints out the user history for reference and the reranked predicted recommendations resulting in lists like the below from the initial query string! \n",
    "\n",
    "```python\n",
    "Showing recommendations for user: 42\n",
    "====================================\n",
    "Movies with high ratings from user\n",
    "--------------------------------\n",
    "Seven (a.k.a. Se7en) (1995) : Mystery|Thriller\n",
    "Silence of the Lambs, The (1991) : Crime|Horror|Thriller\n",
    "Snake Eyes (1998) : Action|Crime|Mystery|Thriller\n",
    "Payback (1999) : Action|Thriller\n",
    "Total Recall (1990) : Action|Adventure|Sci-Fi|Thriller\n",
    "--------------------------------\n",
    "Top movie recommendations\n",
    "--------------------------------\n",
    "Bio Zombie (Sun faa sau si) (1998) : Comedy|Horror\n",
    "Zombieland (2009) : Action|Comedy|Horror\n",
    "ZMD: Zombies of Mass Destruction (2009) : Comedy|Horror\n",
    "Zombie Reanimation (2009) : Action|Comedy|Horror\n",
    "Zombie Women of Satan (2009) : Comedy|Horror\n",
    "The Zombie Diaries (2006) : Action|Horror|Thriller\n",
    "Redneck Zombies (1989) : Horror\n",
    "Hobgoblins 2 (2009) : Horror|Sci-Fi\n",
    "Teenage Zombies (1960) : Horror|Sci-Fi\n",
    "    \n",
    "    \n",
    " \tmovieId \ttitle \tgenres\n",
    "11068 \t47980 \tBio Zombie (Sun faa sau si) (1998) \tComedy|Horror\n",
    "13822 \t71535 \tZombieland (2009) \tAction|Comedy|Horror\n",
    "46049 \t171651 \tRedneck Zombies (1989) \tHorror\n",
    "23643 \t118810 \tZombie Women of Satan (2009) \tComedy|Horror\n",
    "45150 \t169738 \tZombie Wars (2006) \tHorror\n",
    "55180 \t191327 \tTeenage Zombies (1960) \tHorror|Sci-Fi\n",
    "41540 \t161912 \tZombie Night (2003) \tComedy|Horror|Sci-Fi\n",
    "23642 \t118808 \tZombie Reanimation (2009) \tAction|Comedy|Horror\n",
    "14427 \t75404 \tZMD: Zombies of Mass Destruction (2009) \tComedy|Horror\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a53fb",
   "metadata": {},
   "source": [
    "## Improvements and considerations\n",
    "\n",
    "So there is definitely reranking of the films, and it is clear where items like hobgoblins 2 are introduced to replace items which are previously watched by the user. However, I think that there are ways of improving this before it could be put in front of audiences or adapted to other use cases. I think there are a few clear improvements to this model, the first would be a filter on the content for children. Either a \"family friendly\" toggle, or if some percentage of the user's history is made of young audience content then don't show mature content. Both of these are impeded by the fact that there is no age rating data so the programme metadata would need to first be enriched for this content. A second improvement would be to further enrich the items with movie summaries or plot descriptions. These could then be added to the encoded strings to better refine the initial search. This could probably be done through joining to IMBD or Wiki data. This would certainly help when the titles are non-descriptive of the content or are strange non-lexical words, for example \"Jumanji\". Additionally, the length of the candidate items could be varied. When only returning the top 12 items from the SCANN index we end up with a decent list length, 8-10 for all of the users I tested after filtering and reranking. However, for a given query that list is very similar and there is only minimal reranking between users. As the list gets longer more diversity is included in the recommendations, at the very end of the list is some truly unrelated items, but I suspect it would be possible to tune the list length to have greater diversity between lists for users. Whether that behaviour is desirable or whether the nearest things to the query string should be more respected is another matter to balance. Lastly, this reranking only works for users that are present in the CF model's user embeddings. A cold start option should be added for users outside of this, or new users which would likely just be the raw return from the word embeddings query. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
